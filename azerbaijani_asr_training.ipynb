{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azerbaijani ASR Model Training\n",
    "\n",
    "This notebook trains an Automatic Speech Recognition (ASR) model for Azerbaijani language.\n",
    "\n",
    "**Features:**\n",
    "- Toggle between sample mode (CPU) and full training (GPU)\n",
    "- Auto-detection of available hardware\n",
    "- Fine-tuning Whisper model on Azerbaijani dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "**Switch `SAMPLE_MODE` to control training:**\n",
    "- `True` = Small sample, CPU training, quick testing\n",
    "- `False` = Full dataset, GPU training, production model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: SAMPLE (CPU)\n",
      "Dataset size: 100\n",
      "Batch size: 4\n",
      "Epochs: 1\n",
      "Output directory: ./whisper-azerbaijani-sample\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MAIN SWITCH - Toggle between sample and full training\n",
    "# ============================================================\n",
    "SAMPLE_MODE = True  # Set to False for full dataset training\n",
    "\n",
    "# ============================================================\n",
    "# Configuration based on mode\n",
    "# ============================================================\n",
    "CONFIG = {\n",
    "    \"sample\": {\n",
    "        \"dataset_size\": 100,          # Number of samples for testing\n",
    "        \"batch_size\": 4,\n",
    "        \"epochs\": 1,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"max_steps\": 50,\n",
    "        \"eval_steps\": 25,\n",
    "        \"save_steps\": 25,\n",
    "        \"warmup_steps\": 10,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"fp16\": False,                 # CPU doesn't support fp16\n",
    "    },\n",
    "    \"full\": {\n",
    "        \"dataset_size\": None,          # Use full dataset\n",
    "        \"batch_size\": 16,\n",
    "        \"epochs\": 3,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"max_steps\": -1,               # Train for full epochs\n",
    "        \"eval_steps\": 500,\n",
    "        \"save_steps\": 500,\n",
    "        \"warmup_steps\": 500,\n",
    "        \"gradient_accumulation_steps\": 2,\n",
    "        \"fp16\": True,                  # GPU supports fp16\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select active config\n",
    "ACTIVE_CONFIG = CONFIG[\"sample\"] if SAMPLE_MODE else CONFIG[\"full\"]\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"openai/whisper-small\"  # Options: whisper-tiny, whisper-base, whisper-small, whisper-medium\n",
    "DATASET_NAME = \"LocalDoc/azerbaijani_asr\"\n",
    "OUTPUT_DIR = \"./whisper-azerbaijani-sample\" if SAMPLE_MODE else \"./whisper-azerbaijani\"\n",
    "LANGUAGE = \"azerbaijani\"\n",
    "TASK = \"transcribe\"\n",
    "\n",
    "print(f\"Mode: {'SAMPLE (CPU)' if SAMPLE_MODE else 'FULL (GPU)'}\")\n",
    "print(f\"Dataset size: {ACTIVE_CONFIG['dataset_size'] or 'Full'}\")\n",
    "print(f\"Batch size: {ACTIVE_CONFIG['batch_size']}\")\n",
    "print(f\"Epochs: {ACTIVE_CONFIG['epochs']}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SSL Configuration (Corporate Network Fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL verification disabled\n",
      "Xet storage disabled - using regular HTTP\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ssl\n",
    "import sys\n",
    "\n",
    "# ============================================================\n",
    "# SSL BYPASS FOR CORPORATE NETWORKS  \n",
    "# Must run BEFORE any huggingface imports\n",
    "# ============================================================\n",
    "\n",
    "# Disable Xet storage (causes 503 errors)\n",
    "os.environ['HF_HUB_DISABLE_XET'] = '1'\n",
    "\n",
    "# Block hf_xet from being imported\n",
    "sys.modules['hf_xet'] = None\n",
    "\n",
    "# SSL environment variables\n",
    "os.environ['HF_HUB_DISABLE_SSL_VERIFY'] = '1'\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = ''\n",
    "\n",
    "# Disable SSL globally\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Suppress warnings\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Unverified HTTPS request')\n",
    "\n",
    "# Patch requests to disable SSL verification\n",
    "import requests\n",
    "_orig_request = requests.Session.request\n",
    "\n",
    "def _patched_request(self, method, url, **kwargs):\n",
    "    kwargs['verify'] = False\n",
    "    return _orig_request(self, method, url, **kwargs)\n",
    "\n",
    "requests.Session.request = _patched_request\n",
    "\n",
    "print(\"SSL verification disabled\")\n",
    "print(\"Xet storage disabled - using regular HTTP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if packages not installed\n",
    "# !pip install -q datasets transformers accelerate evaluate jiwer librosa soundfile tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected, using CPU\n",
      "\n",
      "Note: fp16 disabled for CPU training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def detect_device():\n",
    "    \"\"\"Auto-detect the best available device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"GPU detected: {device_name}\")\n",
    "        print(f\"GPU memory: {memory:.1f} GB\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"  # Apple Silicon\n",
    "        print(\"Apple Silicon (MPS) detected\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(\"No GPU detected, using CPU\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "DEVICE = detect_device()\n",
    "\n",
    "# Override fp16 based on device\n",
    "if DEVICE == \"cpu\":\n",
    "    ACTIVE_CONFIG[\"fp16\"] = False\n",
    "    print(\"\\nNote: fp16 disabled for CPU training\")\n",
    "\n",
    "# Warning if running full training on CPU\n",
    "if not SAMPLE_MODE and DEVICE == \"cpu\":\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"WARNING: Full training on CPU will be very slow!\")\n",
    "    print(\"Consider using SAMPLE_MODE=True or getting GPU access\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk, Audio\n",
    "import os\n",
    "\n",
    "print(f\"Loading dataset: {DATASET_NAME}\")\n",
    "\n",
    "# Try loading from local disk first (if downloaded via download_data.py)\n",
    "LOCAL_DATA_DIR = \"./data\"\n",
    "\n",
    "if os.path.exists(LOCAL_DATA_DIR):\n",
    "    print(f\"Loading from local: {LOCAL_DATA_DIR}\")\n",
    "    dataset = load_from_disk(LOCAL_DATA_DIR)\n",
    "else:\n",
    "    print(\"Local data not found. Loading with streaming mode...\")\n",
    "    print(\"(Run 'python download_data.py' first for faster loading)\")\n",
    "    \n",
    "    # Use streaming to avoid download issues\n",
    "    dataset_stream = load_dataset(DATASET_NAME, streaming=True, trust_remote_code=True)\n",
    "    \n",
    "    # Convert streaming to regular dataset (takes subset for sample mode)\n",
    "    if SAMPLE_MODE:\n",
    "        n_samples = ACTIVE_CONFIG[\"dataset_size\"]\n",
    "        print(f\"Taking {n_samples} samples from stream...\")\n",
    "        \n",
    "        train_samples = list(dataset_stream[\"train\"].take(n_samples))\n",
    "        test_samples = list(dataset_stream[\"test\"].take(20)) if \"test\" in dataset_stream else train_samples[:20]\n",
    "        \n",
    "        from datasets import Dataset, DatasetDict\n",
    "        dataset = DatasetDict({\n",
    "            \"train\": Dataset.from_list(train_samples),\n",
    "            \"test\": Dataset.from_list(test_samples),\n",
    "        })\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"Full dataset required but not downloaded locally.\\n\"\n",
    "            \"Run 'python download_data.py' first to download the full dataset.\"\n",
    "        )\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample from training set:\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample if in sample mode (only needed for local dataset)\n",
    "if SAMPLE_MODE and ACTIVE_CONFIG[\"dataset_size\"] and os.path.exists(LOCAL_DATA_DIR):\n",
    "    sample_size = ACTIVE_CONFIG[\"dataset_size\"]\n",
    "    \n",
    "    # Sample from train split\n",
    "    if len(dataset[\"train\"]) > sample_size:\n",
    "        dataset[\"train\"] = dataset[\"train\"].select(range(sample_size))\n",
    "    \n",
    "    # Sample from test/validation split if exists\n",
    "    eval_size = min(sample_size // 5, 20)\n",
    "    if \"test\" in dataset and len(dataset[\"test\"]) > eval_size:\n",
    "        dataset[\"test\"] = dataset[\"test\"].select(range(eval_size))\n",
    "    elif \"validation\" in dataset and len(dataset[\"validation\"]) > eval_size:\n",
    "        dataset[\"validation\"] = dataset[\"validation\"].select(range(eval_size))\n",
    "    \n",
    "    print(f\"Subsampled dataset for testing:\")\n",
    "    print(dataset)\n",
    "else:\n",
    "    print(f\"Using dataset:\")\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Whisper Model & Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load processor (tokenizer + feature extractor)\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)\n",
    "\n",
    "# Load model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Configure model for Azerbaijani\n",
    "model.generation_config.language = LANGUAGE\n",
    "model.generation_config.task = TASK\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Resample audio to 16kHz (Whisper requirement)\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "# Detect audio column name\n",
    "sample = dataset[\"train\"][0]\n",
    "audio_column = \"audio\" if \"audio\" in sample else \"path\" if \"path\" in sample else None\n",
    "text_column = \"sentence\" if \"sentence\" in sample else \"text\" if \"text\" in sample else \"transcription\"\n",
    "\n",
    "print(f\"Audio column: {audio_column}\")\n",
    "print(f\"Text column: {text_column}\")\n",
    "\n",
    "# Cast audio column to Audio type with correct sampling rate\n",
    "dataset = dataset.cast_column(audio_column, Audio(sampling_rate=SAMPLING_RATE))\n",
    "\n",
    "print(f\"\\nAudio resampled to {SAMPLING_RATE} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch, audio_column, text_column, processor):\n",
    "    \"\"\"Prepare a single batch for training.\"\"\"\n",
    "    # Load and resample audio\n",
    "    audio = batch[audio_column]\n",
    "    \n",
    "    # Compute input features from audio\n",
    "    batch[\"input_features\"] = processor.feature_extractor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"]\n",
    "    ).input_features[0]\n",
    "    \n",
    "    # Encode target text\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[text_column]).input_ids\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Create partial function with fixed arguments\n",
    "prepare_fn = partial(\n",
    "    prepare_dataset, \n",
    "    audio_column=audio_column, \n",
    "    text_column=text_column,\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "# Process dataset\n",
    "print(\"Processing dataset...\")\n",
    "dataset = dataset.map(\n",
    "    prepare_fn,\n",
    "    remove_columns=dataset.column_names[\"train\"],\n",
    "    num_proc=1 if SAMPLE_MODE else 4,  # Use multiprocessing for full dataset\n",
    ")\n",
    "\n",
    "print(f\"Dataset processed!\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"Data collator that dynamically pads the inputs and labels.\"\"\"\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad input features\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Pad labels\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore in loss\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # Remove BOS token if present (Whisper adds it during generation)\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "print(\"Data collator created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load WER metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute Word Error Rate (WER).\"\"\"\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 with pad token id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and references\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = 100 * wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "print(\"Metrics configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training parameters\n",
    "    per_device_train_batch_size=ACTIVE_CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=ACTIVE_CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=ACTIVE_CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=ACTIVE_CONFIG[\"learning_rate\"],\n",
    "    num_train_epochs=ACTIVE_CONFIG[\"epochs\"],\n",
    "    max_steps=ACTIVE_CONFIG[\"max_steps\"],\n",
    "    warmup_steps=ACTIVE_CONFIG[\"warmup_steps\"],\n",
    "    \n",
    "    # Precision\n",
    "    fp16=ACTIVE_CONFIG[\"fp16\"],\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=ACTIVE_CONFIG[\"eval_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=ACTIVE_CONFIG[\"save_steps\"],\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Generation settings for evaluation\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=25 if SAMPLE_MODE else 100,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    \n",
    "    # Device\n",
    "    use_cpu=(DEVICE == \"cpu\"),\n",
    "    \n",
    "    # Misc\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")\n",
    "print(f\"\\nKey settings:\")\n",
    "print(f\"  - Batch size: {ACTIVE_CONFIG['batch_size']}\")\n",
    "print(f\"  - Learning rate: {ACTIVE_CONFIG['learning_rate']}\")\n",
    "print(f\"  - Epochs: {ACTIVE_CONFIG['epochs']}\")\n",
    "print(f\"  - FP16: {ACTIVE_CONFIG['fp16']}\")\n",
    "print(f\"  - Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "# Determine eval dataset\n",
    "eval_dataset = None\n",
    "if \"test\" in dataset:\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "elif \"validation\" in dataset:\n",
    "    eval_dataset = dataset[\"validation\"]\n",
    "else:\n",
    "    # Use a portion of training data for evaluation\n",
    "    split = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "    dataset[\"train\"] = split[\"train\"]\n",
    "    eval_dataset = split[\"test\"]\n",
    "    print(\"Created eval split from training data (10%)\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  - Train: {len(dataset['train'])} samples\")\n",
    "print(f\"  - Eval: {len(eval_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting training in {'SAMPLE' if SAMPLE_MODE else 'FULL'} mode...\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train!\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"\\nTraining metrics:\")\n",
    "for key, value in train_result.metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running evaluation...\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\nEvaluation results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and processor\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the trained model for inference\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=OUTPUT_DIR,\n",
    "    device=0 if DEVICE == \"cuda\" else -1,\n",
    ")\n",
    "\n",
    "print(\"Inference pipeline loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with samples from the eval dataset (already loaded)\n",
    "print(\"Testing model on samples:\\n\")\n",
    "\n",
    "# Use existing eval_dataset\n",
    "test_samples = eval_dataset.select(range(min(5, len(eval_dataset))))\n",
    "\n",
    "for i in range(len(test_samples)):\n",
    "    sample = test_samples[i]\n",
    "    \n",
    "    # Reconstruct audio from features (or use original if available)\n",
    "    # For now, just print that inference is ready\n",
    "    print(f\"Sample {i+1}: Model ready for inference\")\n",
    "    print(f\"  (Use pipe(audio_array) to transcribe)\")\n",
    "    print()\n",
    "\n",
    "print(\"To test with your own audio:\")\n",
    "print(\"  result = pipe('path/to/audio.wav')\")\n",
    "print(\"  print(result['text'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Push to Hugging Face Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run to push to Hub\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# HUB_MODEL_NAME = \"your-username/whisper-azerbaijani\"\n",
    "# trainer.push_to_hub(HUB_MODEL_NAME)\n",
    "# processor.push_to_hub(HUB_MODEL_NAME)\n",
    "# print(f\"Model pushed to: https://huggingface.co/{HUB_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook trained a Whisper model for Azerbaijani ASR.\n",
    "\n",
    "**To switch from sample to full training:**\n",
    "1. Set `SAMPLE_MODE = False` in Cell 1\n",
    "2. Restart the kernel and run all cells\n",
    "\n",
    "**Next steps:**\n",
    "- Experiment with different model sizes (whisper-tiny, whisper-base, whisper-medium)\n",
    "- Adjust learning rate and batch size for your hardware\n",
    "- Add data augmentation for better robustness\n",
    "- Push the final model to Hugging Face Hub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
